# My Micrograd

A Python implementation of Andrej Karpathyâ€™s [micrograd](https://github.com/karpathy/micrograd) â€” a minimal neural network and autograd engine built from scratch.  
This project follows Karpathyâ€™s YouTube tutorial ["The spelled-out intro to neural networks and backpropagation"](https://youtu.be/VMj-3S1tku0) and serves as my learning exercise in understanding backpropagation, computational graphs, and building models without heavy frameworks.

---

## ğŸ“Œ Features
- Minimal, clean Python code
- `Value` class for scalar values with automatic differentiation
- Basic operations: addition, multiplication, exponentiation, etc.
- Backpropagation from scratch
- Simple neural network implementation

---

## ğŸ“‚ Project Structure
my-micrograd/
â”‚
â”œâ”€â”€ micrograd/ # Core engine
â”‚ â”œâ”€â”€ engine.py # Value class and backprop logic
â”‚ â”œâ”€â”€ nn.py # Simple neural network components
â”‚
â”œâ”€â”€ examples/ # Example usage scripts
â”‚ â””â”€â”€ train.py # Small neural network training example
â”‚
â”œâ”€â”€ README.md # Project documentation
â””â”€â”€ requirements.txt # Dependencies (if any)

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/<your-username>/my-micrograd.git
cd my-micrograd
pip install -r requirements.txt
from micrograd.engine import Value

a = Value(2.0)
b = Value(-3.0)
c = a * b
c.backward()

print(a.grad)  # Gradient of a
print(b.grad)  # Gradient of b
